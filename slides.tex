\documentclass{beamer}
 
\usepackage[utf8]{inputenc}
\usepackage{diagbox}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{makecell}
\usepackage{tikz-dependency}

\usepgfmodule{shapes}
\usepgfmodule{plot}
\usetikzlibrary{decorations}
\usetikzlibrary{arrows}
\usetikzlibrary{shadows}  
\usetikzlibrary{snakes} 
\usetikzlibrary{shapes}
\usetikzlibrary{matrix} 
\usetikzlibrary{positioning} 
\usetikzlibrary{backgrounds}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{mathtools}  % For vcentcolor for defeq
 
\usepackage{siunitx}  % To nicely format numbers
\usepackage{commath}  % To nicely format derivatives
\usepackage{mathtools}  % For vcentcolor for defeq
\usepackage{drawmatrix}
\usepackage{amsthm}  % For theorems
\usepackage[english]{babel}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{placeins}  % For floatBarrier

%_____________________________ DEFINITIONS ________________________________________
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{mydarkgreen}{rgb}{0.0, .4, 0.0}
\definecolor{mybrown}{rgb}{0.59, 0.29, 0.0}
\hypersetup{
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue}

\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\mineq}{\mathrel{-}=}
\newtheorem{thm}{Theorem}  % An environment for writing a theorem.
%\newtheorem{lemma}{Lemma}
\newtheorem*{thm*}{Theorem}  % An environment for writing a theorem.
\newtheorem*{lemma*}{Lemma}
\newtheorem{remark}{Remark}
\newcommand{\defeq}{\vcentcolon=}

\usepackage{xcolor}
\newcommand{\JL}[1]{{\color{purple}JL: #1}}  % A comment from Jon to make in purple. (1) The comment.
\newcommand{\PV}[1]{{\color{green}PV: #1}}  % A comment from Paul to make in green. (1) The comment.

%\newcommand{\cmark}{\ding{51}}%
%\newcommand{\xmark}{\ding{55}}%

\newcommand{\LSE}{\textrm{LSE}}
\newcommand{\Real}{\mathbb{R}}  % The real numbers.
\newcommand{\ep}{\textbf{w}}  % A command for the NN parameters.
\newcommand{\hparam}{\boldsymbol{\lambda}}%\vlambda} % A command for the hyperparameters.
\newcommand{\hp}{\boldsymbol{\lambda}}%\vlambda} % A command for the hyperparameters.
\newcommand{\edim}{m}  % A command for the dimensionality of NN parameters.
\newcommand{\hdim}{n} % A command for the dimensionality of hyperparameters.
\newcommand{\edom}{\textbf{W}}  % A command for the NN parameters.
\newcommand{\xdom}{\mathcal{X}}  % A command for the NN parameters.
\newcommand{\tdom}{\mathcal{Y}}  % A command for the NN parameters.
\newcommand{\hdom}{\boldsymbol{\Lambda}} % A command for the hyperparameters.
\newcommand{\tupparam}{\boldsymbol{\omega}} % A command for a tuple of hyperparameters and NN parameters.
\newcommand{\tupdom}{\boldsymbol{\Omega}} % A command for a tuple of hyperparameters and NN parameters.
\newcommand{\tupdim}{k} % A command for a tuple of hyperparameters and NN parameters.
%\newcommand{\vy}{\boldsymbol{y}}  % A prediction for an input data point and some NN parameters.
\newcommand{\loss}{\mathcal{L}}  % A symbol denoting some loss.
\newcommand{\dataset}{\mathcal{D}}  % A symbol denoting some loss.
\newcommand{\Ltr}{\loss_{\!\text{T}}}  % The training loss.
\newcommand{\Lval}{\loss_{\!\text{V}}}  % The validation loss.

% The following commands don't need to be in notation section - just save typing characters
\newcommand{\hpderiv}[1]{\pd{#1}{\hp}}
\newcommand{\epderiv}[1]{\pd{#1}{\ep}}
\newcommand{\ehpderiv}[1]{\md{#1}{2}{\ep}{}{\hp}{T}}
\newcommand{\eepderiv}[1]{\md{#1}{2}{\ep}{}{\ep}{T}}

\newcommand{\trainGrad}{\epderiv{\Ltr}}
\newcommand{\trainHess}[1]{\eepderiv{\Ltr}}%\!\Ltr\!(\!\hp,\! #1\!)}}
\newcommand{\trainMixed}[1]{\ehpderiv{\Ltr}}%\!\Ltr\!(\!\hp,\! #1 \!)}}
\newcommand{\response}{\ep^{\!*}\!(\!\hp\!)}
\newcommand{\responseJacobian}{\hpderiv{\response}}
\newcommand{\LvalResponse}{\Lval\!(\!\hp,\!\response\!)}
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}

% Acronyms for the paper, so they are easy to switch
\newcommand{\nn}{NN }%{neural network}
\newcommand{\nns}{NNs }%{neural networks}
\newcommand{\NN}{NN }%{Neural Network}
\newcommand{\Nn}{NN }%{Neural network}  % For the start of a sentence
\newcommand{\ho}{HO }%{hyperparameter optimization}
\newcommand{\HO}{HO }%{Hyperparameter Optimization}
\newcommand{\Ho}{HO }%{Hyperparameter optimization} % For the start of a sentence

\input{paper/macros}
\usepackage{mystyle}

\bibliographystyle{unsrtnat}


\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

%Information to be included in the title page:
\title{Efficient Computation of Expectations under Spanning Tree Distributions}

\begin{document}
 
\frame{\titlepage}

\begin{frame}
\frametitle{Dependency Trees}

\begin{itemize}
\item Classical representation of text
\item Similar to phrase structure grammars
    \begin{itemize}
    \item Phrase structure: Labels groups of words
    \item Dependency: Labels relationship between pairs of words
    \end{itemize}
\item Useful if a language has free word order
\item Spanning trees
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{dependency}[theme = simple]
    \begin{deptext}[column sep=0.5em]
      We \& compute \& expectations \& very \& efficiently \\
    \end{deptext}
    \deproot{2}{\large root}
    \depedge{2}{1}{\large nsubj}
    \depedge{2}{3}{\large dobj}
    \depedge{2}{5}{\large advmod}
    \depedge{5}{4}{\large advmod}
    \end{dependency}
    \caption{Example of a dependency tree}
    \label{fig:deptree}
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Efficient Computation of Expectations under Spanning Tree Distributions}
\begin{itemize}
\item A framework for computing expectations (of decomposable functions) over spanning trees
    \begin{itemize}
    \item Unifies previous algorithms
    \item Additionally show large asymptotic
        and empirical speed improvements over particular past implementations
    \end{itemize}
\item Relies on connections between moments and derivatives
    \begin{itemize}
    \item Uses automatic differentiation for easy to implement and efficient algorithms
    \end{itemize}
\item Fun demonstration of inference with automatic differentiation
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Outline}

Goal is to compute expectations over spanning trees
\begin{itemize}
\item Background: Distributions over (spanning) trees
\item Method: Connecting expectations to derivatives
    \begin{itemize}
    \item Use properties of \textbf{our} choice of tree distribution and decomposable functions
    \item Stitch together into efficient algorithms with automatic differentiation
    \end{itemize}
\item Computational complexity
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Distributions over Trees}
Assuming fixed length sentence, with $N$ nodes:
\begin{itemize}
    \item Weighted edges $$(i\,{\xrightarrow[]{\wedge{ij}}}\, j) \tin \edges$$
    \item Trees weights $$\wtree{\tree} \defeq \smashoperator{\prod_{\edgeij\in\tree}} \wedge{ij}$$
    \item Tree probability obtained via normalization
        $$\ptree{\tree} \defeq \frac{\wtree{\tree}}{\wbar},$$
        where
        $$\wbar \defeq \sumtree \wtree{\tree}=\sumtree \prod_{\edgeij \in \tree} \wedge{ij}$$
    \item Computation of $Z$ is tractable despite exponentially many trees in $\st$
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Distributions over spanning trees}
\begin{figure}[H]
    \centering
    \begin{dependency}[theme = simple]
    \begin{deptext}[column sep=0.5em]
      We \& compute \& expectations \& very \& efficiently \\
    \end{deptext}
    \deproot{2}{\large root}
    \depedge{2}{1}{}
    \depedge{2}{3}{}
    \depedge{2}{5}{}
    \depedge{5}{4}{}
    \end{dependency}
    \caption{Example of a dependency tree}
    \label{fig:deptree}
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Matrix-Tree Theorem (MTT)}

\begin{itemize}
\item Compute partition function $Z$ using the graph Laplacian $L\in\R^{N\times N}$:
$Z = \det{L}$.

    \begin{itemize}
    \item Simple (undirected, unweights) graphs: $L = D - A$,
        where $D$ is the degree matrix and $A$ the adjacency
    \item Weighted directed graphs: 
    \begin{equation*}\label{eq:lap}  
        L_{ij} \defeq 
        \begin{cases} 
        \sum\limits_{i'\in\nodes\setminus\{j\}}\wedge{i'j} & \textbf{ if } i=j \\ 
        -\wedge{ij} & \textbf{ otherwise} 
        \end{cases} 
    \end{equation*} 
    \end{itemize}

\item Determinant can be computed in $O(N^3)$ time
\item Flexibility in choice of $L$ depending on example,
    care is needed
\item Results are general for choice of Laplacian $L$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Expectations}
\begin{itemize}
\item Main goal is to compute expectations / totals
    $$\expect{\tree}{f(\tree)} \defeq \sumtree\ptree{\tree}\treefunc{f}{\tree}
        = \frac{1}{Z}\sumtree w(\tree)f(\tree) = \frac{1}{Z} \bar f$$
\item Need to consider every unique tree with nonzero mass unless $f:\st\to\R^F$ decomposes
\item Consider two families of decomposable $f$
    \begin{itemize}
        \item Additively decomposable: Shannon entropy, KL
        \item Second-order additively decomposable: Gradient of entropy, covariance
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Intuition for decompositions}
% Lets try to gain some intuition about decompositions
\begin{itemize}
\item Recall: Tree distribution is a distribution over a binary vector of size $O(N^2)$
\item Compute expectations by
    \begin{itemize}
        \item Storing the relevant parts of trees and their marginal probabilities
        \item Applying $f$ to those parts
    \end{itemize}
\item Consider different levels of decomposability of $f$
    \begin{itemize}
    \item Does not decompose: exponential num trees
    \item Decomposes over pairs of edges: $O(N^4)$ evals
    \item Decomposes over edges: $O(N^2)$ evals
    \end{itemize}
\item Combine $p(\textrm{part})f(\textrm{part})$ to compute expectation
    (unnormalized total using $p(\textrm{part}) = \frac{1}{Z}\tilde{w}_{\textrm{part}}$)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{(Unnormalized) marginals}
\begin{itemize}
\item Unary marginals (prob of one edge appearing in a tree)
    $$p(\edgeij \in d) = \frac{1}{Z}\sum_{\tree\in\inc{ij}}\wtree{\tree}
    = \frac{1}{Z}\totalw{ij}$$
\item Binary marginals (prob of two edges appearing together in a tree)
    $$p(\edgeij,\edgekl \in d) =  \frac{1}{Z}\sum_{\tree\in\inc{ij,kl}}\wtree{\tree}
    = \frac{1}{Z}\totalw{ij,kl}$$

\item Want to show that
    \begin{itemize}
    \item Can decompose totals into sums over marginals (enumerate parts of trees)
    \item Can compute marginals using automatic differentation (cheap gradient principle)
    \item Avoid storing full probability tables (requires low-level tricks, not the focus
        / see paper for details)
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Additively decomposable functions}
\begin{itemize}
\item Allows us to decompose into functions of edges and combine with (unnormalized)
    unary marginals
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{First order totals}
\begin{itemize}
\item Rewrite unary marginal as gradient
\item Rewrwite total as sum over unary parts
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{First order totals: Marginal}
\begin{itemize}
\item A
\item B
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{First order totals: Total}
\begin{itemize}
\item A
\item B
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{First order totals: Algorithm}
\begin{itemize}
\item Rewrite unary marginal as gradient of partition function
\item Rewrwite first order total as sum over unary parts
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Second-order additively decomposable functions}
\begin{itemize}
\item Allows us to decompose into functions of pairs of edges and combine with (unnormalized)
    binary marginals
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Second-order totals}
\begin{itemize}
\item Rewrite binary marginal as Hessian of partition function
\item Rewrite grad of intermediate total as Hessian-vector product
\item Rewrite second-order total as Jacobian-matrix product or Hessian-matrix product
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Second-order totals: Marginal}
\begin{itemize}
\item blah
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Second-order totals: Grad of intermediate total}
\begin{itemize}
    \item asdf
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Second-order totals: Total}
\begin{itemize}
    \item asdf
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Refs}
\bibliography{references,paper/references}
\end{frame}

\end{document}
