\documentclass{beamer}
 
\usepackage[utf8]{inputenc}
\usepackage{diagbox}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{makecell}
\usepackage{tikz-dependency}

\usepgfmodule{shapes}
\usepgfmodule{plot}
\usetikzlibrary{decorations}
\usetikzlibrary{arrows}
\usetikzlibrary{shadows}  
\usetikzlibrary{snakes} 
\usetikzlibrary{shapes}
\usetikzlibrary{matrix} 
\usetikzlibrary{positioning} 
\usetikzlibrary{backgrounds}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algrenewcommand\algorithmicindent{1.0em}%
\renewcommand\algorithmicdo{:}
\renewcommand\algorithmicthen{:}
\newcommand{\rightcomment}[1]{{\color{gray} \(\triangleright\) {\footnotesize\textit{#1}}}}
\algrenewcommand{\algorithmiccomment}[1]{\hfill \rightcomment{#1}}  % redefines \Comment
\algnewcommand{\LineComment}[1]{\State \rightcomment{#1}}
\algnewcommand{\LinesComment}[1]{\State \rightcomment{\parbox[t]{\linewidth-\leftmargin-\widthof{\(\triangleright\) }}{#1}}}

\newcommand{\algorithmicfunc}[1]{\textbf{def} #1 :}
\algdef{SE}[FUNC]{Func}{EndFunc}[1]{\algorithmicfunc{#1}}{}
\makeatletter
\ifthenelse{\equal{\ALG@noend}{t}}%
  {\algtext*{EndFunc}}
  {}%
\makeatother


\usepackage{mathtools}  % For vcentcolor for defeq
 
\usepackage{siunitx}  % To nicely format numbers
\usepackage{commath}  % To nicely format derivatives
\usepackage{mathtools}  % For vcentcolor for defeq
\usepackage{drawmatrix}
\usepackage{amsthm}  % For theorems
\usepackage[english]{babel}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{placeins}  % For floatBarrier

%_____________________________ DEFINITIONS ________________________________________
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{mydarkgreen}{rgb}{0.0, .4, 0.0}
\definecolor{mybrown}{rgb}{0.59, 0.29, 0.0}
\hypersetup{
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue}

\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\mineq}{\mathrel{-}=}
\newtheorem{thm}{Theorem}  % An environment for writing a theorem.
%\newtheorem{lemma}{Lemma}
\newtheorem*{thm*}{Theorem}  % An environment for writing a theorem.
\newtheorem*{lemma*}{Lemma}
\newtheorem{remark}{Remark}
\newcommand{\defeq}{\vcentcolon=}

\usepackage{xcolor}
\newcommand{\JL}[1]{{\color{purple}JL: #1}}  % A comment from Jon to make in purple. (1) The comment.
\newcommand{\PV}[1]{{\color{green}PV: #1}}  % A comment from Paul to make in green. (1) The comment.

%\newcommand{\cmark}{\ding{51}}%
%\newcommand{\xmark}{\ding{55}}%

\newcommand{\LSE}{\textrm{LSE}}
\newcommand{\Real}{\mathbb{R}}  % The real numbers.
\newcommand{\ep}{\textbf{w}}  % A command for the NN parameters.
\newcommand{\hparam}{\boldsymbol{\lambda}}%\vlambda} % A command for the hyperparameters.
\newcommand{\hp}{\boldsymbol{\lambda}}%\vlambda} % A command for the hyperparameters.
\newcommand{\edim}{m}  % A command for the dimensionality of NN parameters.
\newcommand{\hdim}{n} % A command for the dimensionality of hyperparameters.
\newcommand{\edom}{\textbf{W}}  % A command for the NN parameters.
\newcommand{\xdom}{\mathcal{X}}  % A command for the NN parameters.
\newcommand{\tdom}{\mathcal{Y}}  % A command for the NN parameters.
\newcommand{\hdom}{\boldsymbol{\Lambda}} % A command for the hyperparameters.
\newcommand{\tupparam}{\boldsymbol{\omega}} % A command for a tuple of hyperparameters and NN parameters.
\newcommand{\tupdom}{\boldsymbol{\Omega}} % A command for a tuple of hyperparameters and NN parameters.
\newcommand{\tupdim}{k} % A command for a tuple of hyperparameters and NN parameters.
%\newcommand{\vy}{\boldsymbol{y}}  % A prediction for an input data point and some NN parameters.
\newcommand{\loss}{\mathcal{L}}  % A symbol denoting some loss.
\newcommand{\dataset}{\mathcal{D}}  % A symbol denoting some loss.
\newcommand{\Ltr}{\loss_{\!\text{T}}}  % The training loss.
\newcommand{\Lval}{\loss_{\!\text{V}}}  % The validation loss.

% The following commands don't need to be in notation section - just save typing characters
\newcommand{\hpderiv}[1]{\pd{#1}{\hp}}
\newcommand{\epderiv}[1]{\pd{#1}{\ep}}
\newcommand{\ehpderiv}[1]{\md{#1}{2}{\ep}{}{\hp}{T}}
\newcommand{\eepderiv}[1]{\md{#1}{2}{\ep}{}{\ep}{T}}

\newcommand{\trainGrad}{\epderiv{\Ltr}}
\newcommand{\trainHess}[1]{\eepderiv{\Ltr}}%\!\Ltr\!(\!\hp,\! #1\!)}}
\newcommand{\trainMixed}[1]{\ehpderiv{\Ltr}}%\!\Ltr\!(\!\hp,\! #1 \!)}}
\newcommand{\response}{\ep^{\!*}\!(\!\hp\!)}
\newcommand{\responseJacobian}{\hpderiv{\response}}
\newcommand{\LvalResponse}{\Lval\!(\!\hp,\!\response\!)}
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}

% Acronyms for the paper, so they are easy to switch
\newcommand{\nn}{NN }%{neural network}
\newcommand{\nns}{NNs }%{neural networks}
\newcommand{\NN}{NN }%{Neural Network}
\newcommand{\Nn}{NN }%{Neural network}  % For the start of a sentence
\newcommand{\ho}{HO }%{hyperparameter optimization}
\newcommand{\HO}{HO }%{Hyperparameter Optimization}
\newcommand{\Ho}{HO }%{Hyperparameter optimization} % For the start of a sentence

\input{paper/macros}
\usepackage{mystyle}

\bibliographystyle{unsrtnat}


\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

%Information to be included in the title page:
\title{Efficient Computation of Expectations under Spanning Tree Distributions}

\begin{document}
 
\frame{\titlepage}

\begin{frame}
\frametitle{Dependency Trees}

\begin{itemize}
\item Classical representation of text
\item Similar to phrase structure grammars
    \begin{itemize}
    \item Phrase structure: Labels groups of words
    \item Dependency: Labels relationship between pairs of words
    \end{itemize}
\item Useful if a language has free word order
\item Spanning trees
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{dependency}[theme = simple]
    \begin{deptext}[column sep=0.5em]
      We \& compute \& expectations \& very \& efficiently \\
    \end{deptext}
    \deproot{2}{\large root}
    \depedge{2}{1}{\large nsubj}
    \depedge{2}{3}{\large dobj}
    \depedge{2}{5}{\large advmod}
    \depedge{5}{4}{\large advmod}
    \end{dependency}
    \caption{Example of a dependency tree}
    \label{fig:deptree}
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Efficient Computation of Expectations under Spanning Tree Distributions}
\begin{itemize}
\item A framework for computing expectations (of decomposable functions) over spanning trees
    \begin{itemize}
    \item Unifies previous algorithms
    \item Additionally show large asymptotic
        and empirical speed improvements over particular past implementations
    \end{itemize}
\item Relies on connections between moments and derivatives
    \begin{itemize}
    \item Uses automatic differentiation for easy to implement and efficient algorithms
    \end{itemize}
\item Fun demonstration of a general `inference with automatic differentiation'
    recipe
    \begin{itemize}
    \item Compute partition function
    \item Use AD to compute expectations
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Outline}

Goal is to compute expectations over spanning trees
\begin{itemize}
\item Background: Distributions over (spanning) trees
\item Method: Connecting expectations to derivatives
    \begin{itemize}
    \item Use properties of our choice of tree distribution and decomposable functions
    \item Stitch together into efficient algorithms with automatic differentiation
    \end{itemize}
\item Computational complexity
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Distributions over Trees}
Assuming fixed length sentence, with $N$ nodes:
\begin{itemize}
    \item Weighted edges $$(i\,{\xrightarrow[]{\wedge{ij}}}\, j) \tin \edges$$
    \item Trees weights $$\wtree{\tree} \defeq \smashoperator{\prod_{\edgeij\in\tree}} \wedge{ij}$$
    \item Tree probability obtained via normalization
        $$\ptree{\tree} \defeq \frac{\wtree{\tree}}{\wbar},$$
        where
        $$\wbar \defeq \sumtree \wtree{\tree}=\sumtree \prod_{\edgeij \in \tree} \wedge{ij}$$
    \item Computation of $Z$ is tractable despite exponentially many trees in $\st$
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Distributions over spanning trees}
\begin{figure}[H]
    \centering
    \begin{dependency}[theme = simple]
    \begin{deptext}[column sep=0.5em]
      We \& compute \& expectations \& very \& efficiently \\
    \end{deptext}
    \deproot{2}{\large root}
    \depedge{2}{1}{}
    \depedge{2}{3}{}
    \depedge{2}{5}{}
    \depedge{5}{4}{}
    \end{dependency}
    \caption{Example of a dependency tree}
    \label{fig:deptree}
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Matrix-Tree Theorem (MTT)}

\begin{itemize}
\item Compute partition function $Z$ using the graph Laplacian $L\in\R^{N\times N}$:
$Z = \det{L}$.

    \begin{itemize}
    \item Simple (undirected, unweights) graphs: $L = D - A$,
        where $D$ is the degree matrix and $A$ the adjacency
    \item Weighted directed graphs: 
    \begin{equation*}\label{eq:lap}  
        L_{ij} \defeq 
        \begin{cases} 
        \sum\limits_{i'\in\nodes\setminus\{j\}}\wedge{i'j} & \textbf{ if } i=j \\ 
        -\wedge{ij} & \textbf{ otherwise} 
        \end{cases} 
    \end{equation*} 
    \end{itemize}

\item Determinant can be computed in $O(N^3)$ time
\item Flexibility in choice of $L$ depending on example,
    care is needed
\item Results are general for choice of Laplacian $L$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Expectations}
\begin{itemize}
\item Main goal is to compute expectations / totals
    $$\expect{\tree}{f(\tree)} \defeq \sumtree\ptree{\tree}\treefunc{f}{\tree}
        = \frac{1}{Z}\sumtree w(\tree)f(\tree) = \frac{1}{Z} \bar f$$
\item Need to consider every unique tree with nonzero mass unless $f:\st\to\R^F$ decomposes
\item Consider two families of decomposable $f$
    \begin{itemize}
        \item Additively decomposable: Shannon entropy, KL
        \item Second-order additively decomposable: Gradient of entropy, covariance
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Intuition for decompositions}
% Lets try to gain some intuition about decompositions
\begin{itemize}
\item Recall: Tree distribution is a distribution over a binary vector of size $O(N^2)$
\item Compute expectations by
    \begin{itemize}
        \item Storing the relevant parts of trees and their marginal probabilities
        \item Applying $f$ to those parts
    \end{itemize}
\item Consider different levels of decomposability of $f$
    \begin{itemize}
    \item Does not decompose: exponential num trees
    \item Decomposes over pairs of edges: $O(N^4)$ evals
    \item Decomposes over edges: $O(N^2)$ evals
    \end{itemize}
\item Combine $p(\textrm{part})f(\textrm{part})$ to compute expectation
    (unnormalized total using $p(\textrm{part}) = \frac{1}{Z}\tilde{w}_{\textrm{part}}$)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{(Unnormalized) marginals}
\begin{itemize}
\item Unary marginals (prob of one edge appearing in a tree)
    $$p(\edgeij \in d) = \frac{1}{Z}\sum_{\tree\in\inc{ij}}\wtree{\tree}
    = \frac{1}{Z}\totalw{ij}$$
\item Binary marginals (prob of two edges appearing together in a tree)
    $$p(\edgeij,\edgekl \in d) =  \frac{1}{Z}\sum_{\tree\in\inc{ij,kl}}\wtree{\tree}
    = \frac{1}{Z}\totalw{ij,kl}$$

\item Want to show that
    \begin{itemize}
    \item Can decompose totals into sums over marginals (enumerate parts of trees)
    \item Can compute marginals using automatic differentation (cheap gradient principle,
        simple implementation)
    \item Avoid storing full probability tables (requires low-level tricks, not our focus
        / see paper for details)
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Additively decomposable functions}
Allows us to decompose $r:\mcD\to\R^\nR$
into functions of edges and combine with (unnormalized)
unary marginals (over those tree edges)
\begin{equation}
    \rtree{\tree} = \smashoperator{\sum_{\edgeij\in\tree}}\redge{ij},
\end{equation}
with $\redge{ij}\in\R^\nR$
\end{frame}

\begin{frame}
\frametitle{Additively decomposable functions: Marginals}
Marginals are expectations
$$p(\edgeij\in\tree) = \Es{\ptree{\tree}}{1(\edgeij\in\tree)}_{ij} = [\sum_d p(d)r(d)]_{ij}$$
Set
$$\rtree{\tree} = 1(\edgeij\in\tree)$$
Proof
\begin{align*}
1(\edgeij\in\tree) &\teq \sum_{\edgekl\in\tree}1(kl = ij)\\
\Rightarrow \redge{ij} &\teq 1(kl=ij)
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Additively decomposable functions: Shannon Entropy}
Shannon entropy is an expectation
$$\Es{\ptree{\tree}}{-\log \ptree{\tree}}$$
Set
$$\rtree{\tree} = -\log \ptree{\tree}$$
Proof
\begin{align*}
-\log \ptree{\tree} &\teq -\log(\frac{1}{\wbar} \prod_{\edgeij\in\tree} \wedge{ij})\\
&\teq \log\wbar \,{-}\, \sum_{\edgeij \in \tree} \log\wedge{ij}\\
\Rightarrow \redge{ij} &\teq \frac{1}{\nN}\log\wbar\,{-}\,\log\wedge{ij}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{First order totals: Recipe}
\begin{itemize}
\item Rewrite unary marginal as gradient
\item Rewrite total as sum over unary parts
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{First order totals: Marginal}
For any edge $\edgeij$,
\begin{equation}
    \totalw{ij} \defeq \sum_{\tree\in\inc{ij}}\wtree{\tree}=\frac{\partial \wbar}{\partial \wedge{ij}}\wedge{ij},
\end{equation}
where $\inc{ij}\defeq \{ \tree \in \st \mid \edgeij \in \tree \}$
\vspace{2em}
\begin{itemize}
\item Significantly easier to implement via AD than manually
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{First order totals: Marginal derivation}
\begin{align*}
    \totalw{ij} &= \sum_{\tree\in\inc{ij}}\wtree{\tree} & \textrm{(Defn)}\\
    &= \sum_{\tree\in\inc{ij}}\prod_{\edgeijp\in\tree}\wedge{i'j'} & \textrm{(Defn of }\wtree{\tree})\\
    &= \wedge{ij} \sum_{\tree\in\inc{ij}}
        \prod_{\substack{\edgeijp\in \\\tree\setminus \{\edgeij\}}}\wedge{i'j'}
        & \textrm{(Pull out } \wedge{ij})\\
    &=\wedge{ij} \frac{\partial}{\partial \wedge{ij}} \sumtree\prod_{\edgeijp\in\tree} \wedge{i'j'}
        & \textrm{(Sum-prod deriv trick)}\\
    &= \frac{\partial \wbar}{\partial \wedge{ij}} \wedge{ij} & \textrm{(Defn of }Z)
\end{align*}

\end{frame}

\begin{frame}
\frametitle{First order totals: Total}

For any additively decomposable function $\treefuncSig{r}{\nR}$,
\begin{equation}
    \rbar = \smashoperator{\sum_{\edgeij\in\edges}}\totalw{ij}\redge{ij}
        = \smashoperator{\sum_{\edgeij\in\edges}}\frac{\partial \wbar}{\partial \wedge{ij}}\wedge{ij}\redge{ij}
\end{equation}

Proof.
\begin{align*}
    \rbar &= \sumtree\wtree{\tree}\rtree{\tree}  & (\textrm{Defn})\\
    &= \sumtree\wtree{\tree}\sum_{\edgeij\in \tree} \redge{ij} & (\textrm{Defn of }r(d))\\
    &= \sumtree\sum_{\edgeij\in \tree}\wtree{\tree}\redge{ij} & (\textrm{Distributive prop})\\
    &= \sum_{\edgeij\in\edges}\sum_{\tree\in\inc{ij}}\wtree{\tree}\redge{ij} & (\textrm{Commutative prop})\\
    &= \smashoperator{\sum_{\edgeij\in\edges}}\totalw{ij}\redge{ij} & (\textrm{Defn of }\totalw{ij})
\end{align*}

\end{frame}

\begin{frame}
\frametitle{First order totals: Algorithm}

\begin{figure}[H]
\begin{algorithmic}[1]

\Func{$\algCall{\algFirst}{\funcSig{w}{\edges}{\real}, \funcSig{r}{\edges}{\real^\nR}}$}
\LinesComment{Compute first-order total; requires $\bigo{\nN^3\nRs}$ time, $\bigo{\nN^2\!+\!\nR}$ space.
}

\State Compute all $\totalw{ij}$ via AD in $\bigo{\nN^3}$\footnote{
        Constant factor of $\det{L}$ runtime with AD}
\State $\rbar \gets {\displaystyle\smashoperator{\sum_{\edgeij\in\edges}}}
     \totalw{ij} \redge{ij}$ {\color{gray}\algorithmiccomment{ $\bigo{\nN^2\nRs}$ }} \label{line:rbar}
\State \Return $\rbar$
\EndFunc
\end{algorithmic}
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Second-order additively decomposable functions}
\begin{itemize}
\item Allows us to decompose into functions of pairs of edges and combine with (unnormalized)
    binary marginals
\begin{equation}\label{eq:first-order}
    \ttree{\tree} = \rtree{\tree} \stree{\tree}^\top
\end{equation}
\item Outer product of two additively decomposable functions, $\treefuncSig{r}{\nR}$ and  $\treefuncSig{s}{\nS}$
\item $\ttree{\tree} \in \real^{\nR \times \nS}$ is generally a matrix.

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Second-order additively decomposable functions: Binary marginals}
Binary marginals are expectations
\begin{align*}
p(\edgeij,\edgekl \in\tree)
&= \Es{\ptree{\tree}}{1(\edgeij,\edgekl\in\tree)}_{ij,kl}\\
&= \left[\sum_d p(d)r(d)s(d)^\top\right]_{ij,kl}
\end{align*}
Set
$$\rtree{\tree} = 1(\edgeij\in\tree),\qquad\stree{\tree} = 1(\edgekl\in\tree)$$
which we know are additively decomposable
\end{frame}

\begin{frame}
\frametitle{Second-order totals: Recipe}
\begin{itemize}
\item Rewrite binary marginal as Hessian of partition function
\item Rewrite grad of intermediate total as Hessian-vector product
\item Rewrite second-order total as Jacobian-matrix product or Hessian-matrix product
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Second-order totals: Marginal}
Unnormalized binary marginals
\begin{equation}
    \totalw{ij,kl}\defeq\sum_{\tree\in\inc{ij,kl}}\wtree{\tree} 
    =\frac{\partial^2 \wbar}{\partial \wedge{ij}\,\partial\wedge{kl}}\wedge{ij}\wedge{kl}
\end{equation}
where $\inc{ij,kl} \!\defeq\! \{ \tree \!\in\! \st \mid \edgeij \!\in\! \tree, \edgekl \!\in\! \tree \}$
\end{frame}

\begin{frame}
\frametitle{Second-order totals: Marginal}
Same tricks as first-order marginals
\begin{align*}
    \totalw{ij,kl} &= \sum_{\tree\in\inc{ij,kl}}\wtree{\tree} \\
    &= \sum_{\tree\in\inc{ij,kl}}\prod_{\edgeklp\in\tree}\wedge{k'l'} \\
    &= \wedge{ij}\wedge{kl}\frac{\partial^2}{\partial\wedge{ij}\partial\wedge{kl}} \sumtree\prod_{\edgeijp\in\tree}\wedge{i'j'} \\
    &= \frac{\partial^2 \wbar}{\partial \wedge{ij}\,\partial\wedge{kl}}\wedge{ij}\wedge{kl}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Second-order totals: Grad of intermediate total}
Write $\nabla \bar{r}$ in terms of Hessian $\nabla^2 Z$, as a step towards writing
    $\bar{t}$ in terms of $\nabla^2 Z$

\begin{align*}
    &\wedge{ij}\frac{\partial\rbar}{\partial \wedge{ij}} \\
    &= \wedge{ij}\frac{\partial}{\partial\wedge{ij}}\left(\sum_{\edgekl\in\edges}\frac{\partial\wbar}{\partial\wedge{kl}}\wedge{kl}\redge{kl}\right) \\
    &= \wedge{ij}\frac{\partial\wbar}{\partial\wedge{ij}}\redge{ij} + \wedge{ij}\smashoperator{\sum_{\edgekl\in\edges}}\ \ \ \frac{\partial^2\wbar}{\partial\wedge{ij}\partial\wedge{kl}}\wedge{kl}\redge{kl} \\ 
    &= \totalw{ij}\redge{ij} + \smashoperator{\sum_{\edgekl\in\edges}}\totalw{ij,kl}\redge{kl}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Second-order totals: Total}
We can both
\begin{itemize}
    \item Write total $\tbar$ in terms of Jacobian
        $\frac{\partial\rbar}{\partial \wedge{ij}}$
        or gradients $\nabla \rbar_n$
            \begin{itemize}
            \item Run backward for each dimension or $\rbar$: $O(RN^3 + R^2N^2)$
            \end{itemize}
    \item Write total $\tbar$ in terms of Hessian $\nabla^2 Z$
            \begin{itemize}
            \item Hessian is large $O(N^4)$ entries
            \item This takes $O(N^4)$ time, but can be optimized to
                $O(N^2 + RS)$ space
            \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Second-order totals: Total}

\begin{align*}
    \tbar
    &= \sum_{\tree\in\st}\wtree{\tree}\rtree{\tree}\stree{\tree}^{\top} \\
    &= \sum_{\tree\in\st}\wtree{\tree}\rtree{\tree}\smashoperator{\sum_{\edgeij\in \tree}}\sedge{ij}^{\top} \\
    &= \sum_{\tree\in\st}\sum_{\edgeij\in \tree}\wtree{\tree}\rtree{\tree}\sedge{ij}^{\top} \\
    &= \sum_{\edgeij\in\edges}\sum_{\tree\in\inc{ij}}\wtree{\tree}\rtree{\tree}\sedge{ij}^{\top} \\
    &= \sum_{\edgeij\in\edges} \wedge{ij}\frac{\partial}{\partial \wedge{ij}} \!\left(  \sum_{\tree\in\st}\wtree{\tree}\rtree{\tree}\!\right)\!\sedge{ij}^{\top} \\
    &= \sum_{\edgeij\in\edges}\wedge{ij}\frac{\partial \rbar}{\partial \wedge{ij}}\sedge{ij}^{\top}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Second-order totals: Jacobian version}

\begin{figure}[H]
\begin{algorithmic}[1]
\Func{$\algCall{\algSecondVp}{\funcSig{w}{\edges}{\real}, \funcSig{r}{\edges}{\real^\nR}, \funcSig{s}{\edges}{\real^\nS}}$}
\LinesComment{Compute second-order total with gradient-vector products;
requires $\mathcal{O}(\nR(\nN^3 \!+\! \nN^2\nRs \!+\! \nN^2\nSs))$ time, $\bigo{\nN^2\nR \!+\! \nR\,\nS}$ space.
}
\For{$n=1 \ldots \nR$} {\color{gray}\algorithmiccomment{ $\bigo{\nR(\nN^3 + \nN^2\nRs)}$ }} \label{line:rloop}
\State Compute $\nabla\rbar_{n}$ using reverse-mode AD on $[\algFirst(w, r)]_n$
\EndFor
\LinesComment{Requires $\bigo{\nN^2 \nR \nSs}$}
\State \Return ${\displaystyle\smashoperator{\sum_{\edgeij\in\edges}}}
    \frac{\partial \rbar}{\partial \wedge{ij}}\wedge{ij}\sedge{ij}^{\top}$ 
     \label{line:tbar}
\EndFunc
\end{algorithmic}
\end{figure}

\begin{itemize}
\item Recommended approach
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Second-order totals: Hessian version}

\begin{figure}[H]
\begin{algorithmic}[1]

\Func{$\algCall{\algSecondHes}{\funcSig{w}{\edges}{\real}, \funcSig{r}{\edges}{\real^\nR}, \funcSig{s}{\edges}{\real^\nS}}$}
\LinesComment{Compute second-order total by materializing Hessian; requires $\bigo{\nN^4\nRs\nSs}$ time, $\bigo{\nN^2 \!+\! \nR\,\nS}$ space.}
\State Compute all $\totalw{ij}$ via $\nabla Z$
\State Compute all $\totalw{ij,kl}$ via $\nabla^2 Z$ (can compute this efficiently)
\LinesComment{Requires $\bigo{\nN^4 \nRs \nSs}$}
\State \Return ${\displaystyle\smashoperator{\sum_{\edgeij\in\edges}}}\totalw{ij}\redge{ij}\sedge{ij}^\top + {\displaystyle\smashoperator{\sum_{\edgekl\in\edges}}}\totalw{ij,kl}\redge{ij}\sedge{kl}^\top$ \label{line:acum}
\EndFunc

\end{algorithmic}
\end{figure}

\begin{itemize}
    \item Better than previous algo at dealing with sparsity,
        i.e. if $R$ and $S$ are really big and sparse
    \item Can come up with a 3rd algo that has the best of both worlds
        by computing the second term involving the
        Hessian-matrix product more efficiently
\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Refs}
\bibliography{references,paper/references}
\end{frame}

\end{document}
